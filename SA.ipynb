{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1Hl51BlcKBO",
    "outputId": "cfb4ec21-c333-4673-ee13-0d980618d751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-04 14:45:46--  https://dbdmg.polito.it/dbdmg_web/wp-content/uploads/2021/12/DSL2122_january_dataset.zip\n",
      "Resolving dbdmg.polito.it (dbdmg.polito.it)... 130.192.163.163\n",
      "Connecting to dbdmg.polito.it (dbdmg.polito.it)|130.192.163.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18640208 (18M) [application/zip]\n",
      "Saving to: ‘DSL2122_january_dataset.zip’\n",
      "\n",
      "DSL2122_january_dat 100%[===================>]  17.78M  98.2MB/s    in 0.2s    \n",
      "\n",
      "2022-01-04 14:45:47 (98.2 MB/s) - ‘DSL2122_january_dataset.zip’ saved [18640208/18640208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dbdmg.polito.it/dbdmg_web/wp-content/uploads/2021/12/DSL2122_january_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywa0_RkVcvMi",
    "outputId": "51eac27e-8e23-4354-d33b-088377657e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  DSL2122_january_dataset.zip\n",
      "   creating: DSL2122_january_dataset/\n",
      "  inflating: DSL2122_january_dataset/development.csv  \n",
      "  inflating: __MACOSX/DSL2122_january_dataset/._development.csv  \n",
      "  inflating: DSL2122_january_dataset/sample_submission.csv  \n",
      "  inflating: __MACOSX/DSL2122_january_dataset/._sample_submission.csv  \n",
      "  inflating: DSL2122_january_dataset/evaluation.csv  \n",
      "  inflating: __MACOSX/DSL2122_january_dataset/._evaluation.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip DSL2122_january_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.8.1-py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Collecting scikit-learn>=0.24\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: scikit-learn, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.8.1 imblearn-0.0 scikit-learn-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MCtdhH8scz1k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    " accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRoYz7pSn8rn",
    "outputId": "da3c1de7-7aa7-47ec-eb7a-7d878f73c1ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/students/s292129/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/students/s292129/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/students/s292129/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "v0Ouiq35c-KG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('DSL2122_january_dataset/development.csv', index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "fVvVXxrQdCvz",
    "outputId": "b408f28e-f0ae-4343-f832-e0c41984067b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bee22ed6-608c-4eb3-bd16-522b3a230531\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1833972543</th>\n",
       "      <td>1</td>\n",
       "      <td>Mon May 18 01:08:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Killandra</td>\n",
       "      <td>@MissBianca76 Yes, talking helps a lot.. going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980318193</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 06:23:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IMlisacowan</td>\n",
       "      <td>SUNSHINE. livingg itttt. imma lie on the grass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994409198</th>\n",
       "      <td>1</td>\n",
       "      <td>Mon Jun 01 11:52:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yaseminx3</td>\n",
       "      <td>@PleaseBeMine Something for your iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824749377</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 17 02:45:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>no_surprises</td>\n",
       "      <td>@GabrielSaporta couldn't get in to the after p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001199113</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jun 02 00:08:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Rhi_ShortStack</td>\n",
       "      <td>@bradiewebbstack awww is andy being mean again...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bee22ed6-608c-4eb3-bd16-522b3a230531')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-bee22ed6-608c-4eb3-bd16-522b3a230531 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-bee22ed6-608c-4eb3-bd16-522b3a230531');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            sentiment  ...                                               text\n",
       "ids                    ...                                                   \n",
       "1833972543          1  ...  @MissBianca76 Yes, talking helps a lot.. going...\n",
       "1980318193          1  ...  SUNSHINE. livingg itttt. imma lie on the grass...\n",
       "1994409198          1  ...           @PleaseBeMine Something for your iphone \n",
       "1824749377          0  ...  @GabrielSaporta couldn't get in to the after p...\n",
       "2001199113          0  ...  @bradiewebbstack awww is andy being mean again...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ijo5ANgsNdE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT1k-A-0sFwT"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUbj-x_adIfn",
    "outputId": "06b2e20c-ecf9-42da-d5a9-a9424ac72b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 224994 entries, 1833972543 to 2016018811\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   sentiment  224994 non-null  int64 \n",
      " 1   date       224994 non-null  object\n",
      " 2   flag       224994 non-null  object\n",
      " 3   user       224994 non-null  object\n",
      " 4   text       224994 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPORlU7trwSJ",
    "outputId": "dc773a92-286b-4012-c6ff-9d83ce4aac9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NO_QUERY'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.flag.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1HPtk-dr9Ux"
   },
   "source": [
    "There's no need in keep on 'flag' attribute with us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bl949InY9o"
   },
   "source": [
    "Let's see if there's some imbalancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "sz9AH2nVdKWK",
    "outputId": "452d9623-e5a3-4136-d9dd-dd2a2b1e01db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Dataset labels distribution')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_count = Counter(df.sentiment)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(sentiment_count.keys(), sentiment_count.values())\n",
    "plt.title('Dataset labels distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKEYRcqqsnFG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU2QVmUcrpbn"
   },
   "source": [
    "# First benchmark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co8OH4v3vYb-"
   },
   "source": [
    "We'll use only the text as features.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRfZO_NZteRI"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7BS6ym3t-ae",
    "outputId": "148332f0-0da7-479d-99b2-1576b6443487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224994, 156707)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzMK8kzBueDg"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJ_eeQQLuxFz",
    "outputId": "269577c3-2add-4af1-d4ef-76420816c07b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((179995, 156707), (179995,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CDWUyf2rovu",
    "outputId": "adb487ec-f297-4087-dad4-31e03cae87da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.702926731705149\n",
      "f1 score:  0.7437509584419568\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyrf3kvxslYD"
   },
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "397uYQ_gv-Mf"
   },
   "source": [
    "Lemmatization and Stopwords elimination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "B2-38h4LnXeM",
    "outputId": "5c303bc8-7807-4d6a-be8e-dc6e4a3e3c88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6c16b9de2a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlemmaTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlemmaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6c16b9de2a65>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "  def __init__(self):\n",
    "    self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "  def __call__(self, document):\n",
    "    lemmas = []\n",
    "    for t in word_tokenize(document):\n",
    "      t = t.strip()\n",
    "      lemma = self.lemmatizer.lemmatize(t)\n",
    "      lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer, stop_words=stopwords.words('english'))\n",
    "X_tfidf = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yejE8um3yruM"
   },
   "source": [
    "Performances after lemmatization and stopwords elimination:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5xxSZX2zEBP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvRvjuMm1TC4",
    "outputId": "2db5ed24-aa55-4461-e46e-afd4e714884f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179995, 173979)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCLFA5CfyWHn",
    "outputId": "0f114db2-7738-4ff6-e31d-267f255199c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6928820640458677\n",
      "f1 score:  0.7392256019322212\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cQW-gx1Dh21"
   },
   "source": [
    "Let's try stopword removal only:\n",
    "\n",
    "you can make a set of stopwords and remove some of the words using the remove function.\n",
    "\n",
    "stoplist = set(stopwords.words(\"english\")\n",
    "stoplist.remove('Not').\n",
    "\n",
    "You can then use this set to filter out the list of stopwords you want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOxjiobP7ACP"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X_tfidf = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlQE1WWZ7ACP",
    "outputId": "e3765437-a749-4c95-abc5-972fda50b1dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224994, 156563)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeqbPWiO7ACP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqYw4dx_7ACP",
    "outputId": "ccddcb0a-7686-45c5-dea8-6c0d6ce7fc42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((179995, 156563), (179995,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1mYES-D7ACP",
    "outputId": "5106c9ba-8732-498a-db2a-ea06480d05ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7093268739305318\n",
      "f1 score:  0.7520191104538733\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKc53jWQDltJ"
   },
   "source": [
    "It's clear that lemmatization worsen the performances.. this can happen because abbreviations, like punctuation, can be relevant to sentiment detection.\n",
    "The problem is that default tokenization used by TfidfVectorizer explicitly ignores all punctuation.\n",
    "\n",
    "We can modify **token_pattern** in Tfidf_vectorizer to be any character except one or more whitespaces:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_KejCLPGVxz",
    "outputId": "b487b28d-d17f-4c6b-83ef-640bf5faa4ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 263344)\n",
      "Accuracy:  0.7001488921976043\n",
      "f1 score:  0.7436594030814827\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), token_pattern=r'[^\\s]+')\n",
    "X_tfidf = vectorizer.fit_transform(df.text)\n",
    "print(\"Shape after tf-idf: \", X_tfidf.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PFUU0RWOK6f"
   },
   "source": [
    "If we want to modify the hyperparameters of the tf-idf model, we should tune \n",
    "\n",
    "\n",
    "1.   Term frequency weighting. Recall that the term frequency is the normalized count of terms in a given document. This value can be set to:\n",
    "• b - binary,\n",
    "• t or n - raw,\n",
    "• a - augmented,\n",
    "• l - logarithm,\n",
    "• d - double logarithm,\n",
    "• L - log average.\n",
    "\n",
    "2.   Document frequency weighting. Recall that the document frequency is the number of documents in a corpus that contain a given term. This value can be set to:\n",
    "• x or n - none,\n",
    "• f - idf,\n",
    "• t - zero-corrected idf,\n",
    "• p - probabilistic idf.\n",
    "3.   Document normalization. Each document is normalized so that all document vectors are turned into unit vectors. In doing so, we eliminate all information on the length of the original document; this masks some subtleties about longer documents. First, longer documents will — as a result of containing more terms — have higher term frequency values. Second, longer documents contain more distinct terms. The document normalization can be set to:\n",
    "• x or n - none,\n",
    "• c - cosine,\n",
    "• u - pivoted unique,\n",
    "• b - pivoted character length.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-T_Js6cVzBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT8NrS0uSCnY"
   },
   "source": [
    "Other preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "21_AA7sISFAw"
   },
   "outputs": [],
   "source": [
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused',\n",
    "          '$_$': 'greedy','@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused','<(-_-)>': 'robot', 'd[-_-]b': 'dj', \n",
    "          \":'-)\": 'sadsmile',';)': 'wink',';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "punctuations = \\\n",
    "\t[\t('__PUNC_EXCL',\t\t['!', '¡', ] )\t,\n",
    "\t\t('__PUNC_QUES',\t\t['?', '¿', ] )\t,\n",
    "\t\t('__PUNC_ELLP',\t\t['...', '…', ] ) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Jfa3b7kYSLkC"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    processedText = []\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"(http|https|ftp)://[a-zA-Z0-9\\\\./]+\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE)\n",
    "\n",
    "    \n",
    "    for tweet in text:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL', tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)\n",
    "        # Replace punctuation.\n",
    "        for (key, parr) in punctuations:\n",
    "          for punc in parr:\n",
    "            if punc in tweet:\n",
    "              tweet = tweet.replace(punc, key)\n",
    "        # Replace characters repetitions with two words\n",
    "        tweet = re.sub(rpt_regex, replace_repet, tweet)\n",
    "        # Replace all non alphabets.\n",
    "        #tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        #tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        processedText.append(tweet)\n",
    "        \n",
    "    return processedText\n",
    "\n",
    "\n",
    "def replace_repet(match):\n",
    "\treturn match.group(1)+match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AihJPy2NSXCG"
   },
   "outputs": [],
   "source": [
    "X_preproc = preprocess(list(df['text']))\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fkea2KPcScbX",
    "outputId": "85228131-9220-4742-e807-ff3e31065b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 74271)\n",
      "Accuracy:  0.7055045667681504\n",
      "f1 score:  0.7492810655365522\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X_preproc)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEKhda5JV_gs",
    "outputId": "49f48a88-6604-4155-b6d7-8b0fbe9bc4f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 74271)\n",
      "Accuracy:  0.7053934531878486\n",
      "f1 score:  0.7505973097544917\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), use_idf=False)\n",
    "X = vectorizer.fit_transform(X_preproc)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlWEI-oFUsIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocCo4EWVWyyz"
   },
   "source": [
    "Let's try linearSVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Y4KSsBgS6R2",
    "outputId": "139ea9c3-bbd7-4141-d6a8-286ef51abac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 74271)\n",
      "Accuracy:  0.77397275495011\n",
      "f1 score:  0.8109725500399576\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), use_idf=False)\n",
    "X = vectorizer.fit_transform(X_preproc)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDjNgjHGa9X2"
   },
   "source": [
    "Let's try Logistic regression with 0.1% test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4MEpQjy9G-l",
    "outputId": "b2a53152-aa89-413e-a16a-dbc697a4acf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7811111111111111\n",
      "f1 score:  0.8190070192201683\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(df.text)\n",
    "y = df.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "reg = LogisticRegression(max_iter = 1000, n_jobs=-1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqObsx_5Yj_Y",
    "outputId": "c37e090d-93fa-4750-d3c7-ce9bfb2f33d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 114534)\n",
      "Accuracy:  0.7741505366785929\n",
      "f1 score:  0.8133619818926413\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X_preproc)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "log_clf.fit(X_train,y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes talking helps a lot going through it there s no judgment most sentences end with you know what i mean',\n",
       " 'sunshine livingg itttt imma lie on the grass listening to oasis reading burnt already',\n",
       " 'something for your iphone',\n",
       " 'couldn t get in to the after party',\n",
       " 'awww is andy being mean again now i want maccas',\n",
       " 'i m getting so anxious about tomorrow help',\n",
       " 'the doctors and masters are screwed then',\n",
       " 'barely been used',\n",
       " 'hahah it can t work on livejournal',\n",
       " 'ugh sorry ur sick fresh fruit drinks vits e c b echinacea golden seal rest is nice',\n",
       " 'boo i hate facebook',\n",
       " 'i m going for a while now we re off out for a few hours c yah and thanks again for all the birthday wishes',\n",
       " 'yeah tried sleeping couldn t so supernatural instead',\n",
       " 'i always remember how to spell rhythm with rhythm has your two hips moving',\n",
       " 'oh awesome thanks for clarifying and for the kind words we should meet up here at some point',\n",
       " 'how s it going mate xx',\n",
       " 'is going to get herhaircut soon going to get a fringe methinks',\n",
       " 'i ve been amazing just a little down cuz my bff is sad but besides that im fiiinee so what r u doing',\n",
       " 'right i m off to bed then tomorrow i m buying lottery tickets so i won t even need a decent job night x x x',\n",
       " 'bugger you got to the gaming blog before me',\n",
       " 'haha i m good i missed twitting haha',\n",
       " 'the doggie park was lovely all behaved fairly well fairly he he',\n",
       " 'i could cry forgot to log out of jcc at home so now i can t get on it here at work',\n",
       " 'hello don t suppose you re stopping but it s nice to see ya',\n",
       " 'emily and ellie came over for like mins hahaa imissellie',\n",
       " 'awwh noo lets put a giant cuddy toy in there p',\n",
       " 'mom n dad just got done making homemade ice cream yum',\n",
       " 'i have a cold',\n",
       " 'apparently it won t',\n",
       " 'not alot though hahah i never go on yahoo anymore hows jessica',\n",
       " 'i am lost please help me find a good home',\n",
       " 'ohmygod i have thhe worst headache',\n",
       " 'glad u had fun on saturday i had fun too oh and hopefully cleo gets better soon',\n",
       " 'oh you must be insomiac demi well i am and i always find it hard to sleep haha loving jbros album',\n",
       " 'get followers a day using www tweeteradder com once you add everyone you are on the train or pay vip',\n",
       " 'rox good morning your majesty i been blessed with another day in your world',\n",
       " 'i m gonna be ill if joan rivers wins she melissa are so annoying',\n",
       " 'mr nimoy did not attend the screening afaik',\n",
       " 'song i like to be in usa for see your marathon you re aweosome girl',\n",
       " 'omg barrett needs to do the view',\n",
       " 'why don t you tell her to go with you she d see there s nothing to worry about jonaskevin',\n",
       " 'no i mean the jjonas movie',\n",
       " 'r haha second one i also have cupcakes but i am not nomming them at the moment lol',\n",
       " 'they were extra cautious of touching me',\n",
       " 'omg its thundering and raining really bad now',\n",
       " 'well as long as you have trees yeh definately need to go find one maybe after the crutches tho p',\n",
       " 'not embarrassing at all it s perfectly ok for you to be you',\n",
       " 'well done on your wife to be able to breast feed lucky boy is he your first',\n",
       " 'nooooooo dont lose anymore',\n",
       " 'are you online with this username now',\n",
       " 'sorry u have to work have same free tasting every saturday cu tuesday wednesday',\n",
       " 'here you go',\n",
       " 'wow thanks for the comments i will very soon write lots more just getting things organized for the moment thanks',\n",
       " 'it s one of my favourites it s joint with you belong with me',\n",
       " 'oh yes all caps is shouting tweet softly and carry a big stick',\n",
       " 'smooches my back is still crappy poopoo hows the jaw',\n",
       " 'the pink zebra one i don t care so much about the zebra but i love the rest of it',\n",
       " 'oh now i know thanks',\n",
       " 'connecting and sharing fb me at www facebook com belindastarnes i luv sharing thoughts learning from tweeps',\n",
       " 'ah true probably not going to get away with elderflower champagne either i suspect',\n",
       " 'i love that the word freakin has now appeared in a bootsnall today quote',\n",
       " 'ohhh that sounds fun',\n",
       " 'also i didn t expect spymaster dm spam from you',\n",
       " 'that hurt just fucked me harddddd lol',\n",
       " 'hope you feel better soon and you had a lovely weekend',\n",
       " 'panera bread s internet access is lame it block all the twitter tiny url as porn talk about over protecting',\n",
       " 'i need a soda',\n",
       " 'i suppose i should write the action notes for the opd that i really should have done a month ago but can t be bothered',\n",
       " 'omg nooooo boston',\n",
       " 'i am on gravity but is slow and overload twitter',\n",
       " 'thank you i didn t know where the l altro bucca is',\n",
       " 'the unbalancing that d be the caps lock then in a bad mood with microsoft taking it out on twitter in a bad mood generally',\n",
       " 'happy father s day i miss my dad',\n",
       " 'the young ones don t like working weekends they are prime shifts i love them',\n",
       " 'on the computer alllllllllllllllll day',\n",
       " 'i seriously freaking hate labels especially towards girls just because a girl is sexy and confident with themselfs they are wh s wtf',\n",
       " 'my dog is gonna be he s getting old he has grey hairs on his little face',\n",
       " 'i have somewhere i wanna bring you tmrw before the meeting',\n",
       " 'tweetdeck keeps getting me excited',\n",
       " 'hehe you got it in one cool',\n",
       " 'well it s pretty stressful especially today',\n",
       " 'lol i can gauge you by how you hold your cup',\n",
       " 'vinyls are so expensiveeeeee',\n",
       " 'me and my four year old next door neighbor are wearing the same dress today i may have been more excited than her about this fact tho',\n",
       " 'izquierda comes from the basque eskerra so no wonder then',\n",
       " 'because people discuss what they are passionate about including changing how people parent clothdiapers',\n",
       " 'well at least unlike me both options don t apply p s congrats on the former part',\n",
       " 'the one i cant have is the one that i want the most',\n",
       " 'storm ty for the ff',\n",
       " 'race t f ll wersz i l se l l',\n",
       " 'yikes sorry to hear abt the laptop still haying you mama',\n",
       " 'jack is glad that his leg is getting better but sad that paul has gone back to newcastle',\n",
       " 'anea our pleasure thanks for gracing us with your space goat awesomeness',\n",
       " 'omg dustin hoffman on jross talking about sod beating rafa why is this haunting me',\n",
       " 'i miss s tweets',\n",
       " 'good in this times it s hard to make profit at all',\n",
       " 'woo i totally just got a hater telling me i suck and cant spell i think ive made it',\n",
       " 'nothing from the window no time to go down and see the aftermath',\n",
       " 'back home plans changed now watching french open women s final aus vs wi alternatively wid frnds',\n",
       " 'davies but no problemooo shang s fifty steps away from the hotel let s set a twitter meeting tomorrow with pia and carissa lol',\n",
       " 'enjoying the freedom without a boss today',\n",
       " 'cya lol i might buy it after my exams who knows anything can happen',\n",
       " 'give him a squeeze for me',\n",
       " 'yes we are',\n",
       " 'always a pleasure',\n",
       " 'the gang s all here well no there are quite a few missing but u know what we mean',\n",
       " 'okay i love nick jonas but i think he s a manwhore lmfao',\n",
       " 'the sims is very confusing me and chloe cant figure out how to move house o any help',\n",
       " 'u mean love songs yeah it s the only one i have',\n",
       " 'okay goodnight',\n",
       " 'i have pins and needles in my right foot',\n",
       " 'aah cas is a bit out of the way beer might have to do though sadly either that or vodka lol',\n",
       " 'no lotusnotes longer hours no date',\n",
       " 'i got a whole hours of sleep last night because for whatever reason i couldn t fall asleep i m sure i m gonna be feeling the eff',\n",
       " 'wockeez i wanna say thanks for brining hiphop to a nkotb show i m a hiphop junkie loose my mind when y all hit the stage thanks',\n",
       " 'on the way to bed',\n",
       " 'ugh my hair wants to be robert smith today',\n",
       " 'sam adam s tail just caught fire the house smells like burning cat hair',\n",
       " 'i wake up to something i did not want to read',\n",
       " 'hi tipsy new nickname it ll prob change hope you got your sunnies out today',\n",
       " 'i like to think of jesus as a mischevious badger hahahaha',\n",
       " 'you work on your paper i ll work on writing about my close encounters when you re finished writing you can read mine',\n",
       " 'im good really bored lol heard judy on the radio haha',\n",
       " 'any time',\n",
       " 'lol just jokin u it is ok some blondes r ok though just so you know',\n",
       " 'jackie yeah it s jackie lol',\n",
       " 'saffron',\n",
       " 'on i was feelin ur pics u gona make it',\n",
       " 'oooh weimaraner they are soo pretty my poor little dog has to have surgery next week',\n",
       " 'at work now lovely day going away tonight going to miss her wish i could be at home to enjoy the weather',\n",
       " 'my mum is getting her stuff ready she s leaving tomorrow booo im not gonna see her till christmas or maybe halloween',\n",
       " 'all good russ good luck with ur exams and i m sure i ll catch you another time',\n",
       " 'why is nambu always randomly scrolling down when it update the timeline with new tweet',\n",
       " 'hello tweeples my day s been amazingly good so far',\n",
       " 'on our way to solvalla to watch the horses sweden is sunny',\n",
       " 'coz i ll be in syd then',\n",
       " 'no i haven t',\n",
       " 'one choice at a time you can make a difference for our planet',\n",
       " 'hey chicken how are you having a good weekend its lush outside',\n",
       " 'whoah crap that was a mistake do not put the three letters together in a tweet im el im just got overwhelmed with follow bots',\n",
       " 'heading to cafe to meet freak face done hours straight revision this morning aah tmro',\n",
       " 'in a fruity mood i have strawberries and a orange',\n",
       " 'do you think viagra spammers are hard up boom boom yes i did go there',\n",
       " 'yup it was good i was very pleased wolverine',\n",
       " 'aw cute hell s kitchen dude is no longer cute tobaccochewingdick',\n",
       " 'do your arms and shoulders hurt from writing so much mine do',\n",
       " 'm i havent seen it either',\n",
       " 'cus bangin on me damn lmao',\n",
       " 'clean me',\n",
       " 'o just got off work getting ready to leave for indpls mini marathon wish me luck',\n",
       " 'i joined ficly and finished that challenge',\n",
       " 'hahhaa yay i know you should update it after your summer class lalalallaa',\n",
       " 'ladette to lady looks interesting this year should be a laugh',\n",
       " 'was just on radio',\n",
       " 'wait till u read her stuff omg i suspect you will fall about laughing at least cm is not formally spawning himself',\n",
       " 'well that is awfully generous of you thank you',\n",
       " 'the big bang theory series finale made me feel quite bad for leonard and penny',\n",
       " 'i want a car of my own',\n",
       " 'yep he s eating it the dirty bastard right gotta look busy the last mins later y all in out this bitch lovin ya work',\n",
       " 'if i was there i would i misses you',\n",
       " 'working on another painting will list and post here soon',\n",
       " 'nightttt sleep well xxx',\n",
       " 'why cant we have a football tournament like epl in india',\n",
       " 'hello legs and hello billam',\n",
       " 'myweakness hmmm lots lots of shopping it s addictive',\n",
       " 'everytime still holds horrible horrible memories for me though',\n",
       " 'lol not productive here in us',\n",
       " 'i feel so fucking ill i blame the horrible bright lights in the drama studio',\n",
       " 'nickj i don t like it because you can t hear the lyrics so i can t sing it lol',\n",
       " 'thanks i ll be s ssssshh how old you',\n",
       " 'yeah ughugh if it doesn t get fixed tonight we might just change the raid nights',\n",
       " 'life not much just on the train i m assuming ur watchin the game',\n",
       " 'i can t wait to see what this is going to entail i can imagine how amazing it s gonna be',\n",
       " 'i have both her cds and know them both by heart really tal you think you can intro me to country music',\n",
       " 'i know im such an epic fail',\n",
       " 'eww jus had the worse dream ever',\n",
       " 'would you please follow me back',\n",
       " 'only minutes away but i leave in min so can t help you',\n",
       " 'okay now im done hehe good night now its for reall',\n",
       " 'i m actually polish and irish corned beef on st patty s day and soda bread',\n",
       " 'he s made to play those kinda insane characters hehe',\n",
       " 'i like her',\n",
       " 'aha yea someone came knocking then my mom told him to go out the back cuz a bunch young boys were out in front',\n",
       " 'oh my gosh gila really i loveeee lifehouse bella loves them too which lifehouse songs are your fave',\n",
       " 'eating on the balcony with mum and sister weather s amazing summer is deff on its way',\n",
       " 'can you play i wish by jordan knight',\n",
       " 'it looked as if you were having a ball too',\n",
       " 'i neeeed to cut me nails',\n",
       " 'not a good way to spend your holidays though bill',\n",
       " 'which reminds me i took better photos of the necklaces and i think you will like them i ll post this week and let you know',\n",
       " 'i d consider enrolling in your class as you re very passionate about teaching if only it wasn t in california dan',\n",
       " 'wouldn t be much eating in the wee things',\n",
       " 'my vlog is processing now posting soooon',\n",
       " 'lol i was on cam with it last night my boyfriend said i look like sho nuff',\n",
       " 'any apps out there that will take my csv list of keywords and placement and let me track them over time',\n",
       " 'yes i know i think i ll just wait for tomorrow and see what happen',\n",
       " 'i r scared',\n",
       " 'why does stomach ache have to hurt so much',\n",
       " 'goodmorning yawn hmm physics',\n",
       " 'sorry for this late response oh i love the place where i ve been it s soooo nice you should try it when you visit phil',\n",
       " 'yeah i wanna hear her new album too she is amazing and john mayer too',\n",
       " 'love sleeping in sundays woke up near am and have spent the last hour drinking coffee and reading news perfectly lazy',\n",
       " 'o do i have say hey to continue been followed ily',\n",
       " 'photo minneapolis the place i call home',\n",
       " 'veronica awww so do i they suck',\n",
       " 'i had one of the most frustrating days at work you have totally made my day thanks sweetie',\n",
       " 'watching southpark and waiting for a friend to come on c',\n",
       " 'and great to see a team with mcl tech partnership also beating mcl keep up the good work at fi',\n",
       " 'i ll be back soon home that is',\n",
       " 'i m so sure i told her about it i can t believe she forgot waaaaaah and she s my friend i m over it lol',\n",
       " 'o yes house of payne i can t believe the little boy is moving i love him i dont watch meet the browns my mom said its funny',\n",
       " 'last night couldn t sleep so i was a space invader and woke up my best friend and we had a cool talking session',\n",
       " 'looking forward to the apprentice im recording it bcos im gonna watch a film im recording all this weeks lw too xo',\n",
       " 'gallagher that is delightful',\n",
       " 'i will need to try them x',\n",
       " 'ahhhh where is the love wtf on about channels at once aswell',\n",
       " 'trying real hard to stay awake til bedtime jetlag ftw',\n",
       " 'i just voted for you xox',\n",
       " 'might find myself a new friend since thinks im soooo lame',\n",
       " 'everyone is here we re off to party now',\n",
       " 'oooh you know how to hurt xx',\n",
       " 'something save us more bnp councillors nice one england',\n",
       " 'my dvr is broked they replaced it with a broked one comcast sucks i haz to call them out again',\n",
       " 'reality is always shocking',\n",
       " 'going to las vegas next wk and will be visiting canyon ranch spaclub',\n",
       " 'oh man has it been an hour an a half already i better go to bed safe trip talk to yall later kim',\n",
       " 'haha maybe you could enter next year',\n",
       " 'well we didn t get fat overnight although i swear i did lol and it won t come off quickly',\n",
       " 'it was im sorry ay im so depressed now im killin in side sorry',\n",
       " 'you re welcome',\n",
       " 'bride wars was suchhh a cute moviee if you haven t seen marley me that s amazing yes man is a good laugh have fun',\n",
       " 'so i haven t gotten twitter updates on my phone all day i need to fix that heck idk if me texting an update is even working nighttt',\n",
       " 'i told em that the english have a special quality when it comes to cussing welcome to the fun dutch',\n",
       " 'absolutely i follow everybody who follows me i have an open door policy don t be shy tweeps',\n",
       " 'thanks g to followfriday',\n",
       " 'sanctuarysunday it s our mission to make sanctuary a trending topic for the day lots of people on board for it join in',\n",
       " 'i wish i could help but i have no idea about logo s sorry',\n",
       " 'ok get on your feet let s dance even though the song may suggest you don t want to scissor sisters don t feel like dancing',\n",
       " 'ello followers nuevo heller how you deeewwwrin',\n",
       " 'kill boy dont tell me that',\n",
       " 'yea but i want you to be special and if your everybodies cali girl than its not special',\n",
       " 'thanks pooter it s been difficult catching up we ll see if i shoulda just taken the leave of absence im trying hard',\n",
       " 'a brazil fan a girl they know whent to the tv they were filming and saw bruises on dannys arm',\n",
       " 'the follow up post is up but its less offending and more sarcastic this time',\n",
       " 'i knoww he makes me giggle i loves him',\n",
       " 'please vote for my puppy lucy',\n",
       " 'ah well some blood is better than no blood',\n",
       " 'aww sounds like you had a rough day hope it gets better for you',\n",
       " 'so far i always got there safely the trick is to really believe it when you say or think it i m not helping am i',\n",
       " 'yeah realisitically we all know it will happen but we re all in denial about it we re happy being in denial',\n",
       " 'i m going strawberry picking tomorrow then i can make jame',\n",
       " 'catch y all after dmv ugh',\n",
       " 'mcfox',\n",
       " 'lol candyman and the urban legend it s about is one of the scariest premises ever just thought i d share that',\n",
       " 'i will never stop following you dave you got the best jams around some of the stuff you play i hadn t heard in years',\n",
       " 'standing next to papa nd i just realised that i m taller than him lol',\n",
       " 'e zem ya wallad am here waiting for someone to go eat with me and there is none',\n",
       " 'lawrence ironically one of the youth group gave me a leather iphone case tuesday eve that his dad didn t want oh the irony',\n",
       " 'cable guy l be here this afternoon do i ever miss u all cell doesnt cut it i walkd new house so yay me lol talk soon xo',\n",
       " 'just paid off a ticket i got in georgia that i ve been avoiding',\n",
       " 'never said she was',\n",
       " 'omg my myspace has gone blank shittttt i have to start all over again',\n",
       " 'thanks for the follow',\n",
       " 'i gave the game away',\n",
       " 'max is wishing he had a momma',\n",
       " 'took this when i was at the cinema i want to watch this one',\n",
       " 'what will u do if u re abt to get ur twitter time just to find that ur electricity went off one day and internet connection the next',\n",
       " 'is very very tired and really doesn t want to go to maths',\n",
       " 'aww i love chuck i can t wait for it to come back',\n",
       " 'noooooo big hug to u i d luv be yr nancy yr sid get on bb im so we can chat plse xxxx',\n",
       " 'washing sucks don t it yeah general shop and lunch in bakers oven should be home by about pm lol',\n",
       " 'it s so annoying when i want to go see the sats and in concert but i know i ll have to pay',\n",
       " 'exactly how many stalkers do you have lol morning btw',\n",
       " 'thanks and doubly so i hope you re having a good one too i am thanks',\n",
       " 'daughtry does know that the fans do plan their vacations around tour stops so we need info on these summer shows like yesterday lol',\n",
       " 'c n peoples im here finally sorry im so late couldnt be helped',\n",
       " 'hmm gotta say i m intrigued by project natal',\n",
       " 'oh lol i think another macbook cos it s cost effective and cool',\n",
       " 'yeah i have driven one before they are amazing can t wait to get one excited',\n",
       " 'i haven t seen it yet that s just on previews it comes out here on the th',\n",
       " 'mmmm definitely time for lunch then i guess',\n",
       " 'back in texas we call ice cream frozen cow juice',\n",
       " 'if you need any help just let me know i try to asist you',\n",
       " 'but you re the bestestest i m sure it ll be ok',\n",
       " 'wow you are really excited about ihop too excited lol i can tell you ve never been it s not that great',\n",
       " 'i said legs not leg',\n",
       " 'i just watched the shittiest play ever lol falling asleep want to nap with hubby',\n",
       " 'for my previous group i even made wine labels to put on clean skins to give as gifts to speakers',\n",
       " 'lu zomg is on something',\n",
       " 'hey thanks for the followfriday recommend',\n",
       " 'time to sleep early good night friends take care always',\n",
       " 'oh yeah add me on youtube',\n",
       " 'i have a headache',\n",
       " 'going to do my french test now wish me luck',\n",
       " 'he s waiting for you with his mates',\n",
       " 'it rained here as well missed my sunny boy all day long lol',\n",
       " 'sims cool i love sims and i can t hardly wait for',\n",
       " 'lmao wtf say it ain t so sheriff',\n",
       " 'how can i possibly not have a good afternoon',\n",
       " 'hi caleb australia well sydney is raining and cold i make no sense my head hurts hope all is well',\n",
       " 'galing yan sa chasing the dream di ko pa napapanuod i have no time now',\n",
       " 'anna was getting ready to do a tandem skydive with her brother albert',\n",
       " 'i jus love c h that kid is everythin tat i was not but always wanted to be as a kid i jus cant stop readin it',\n",
       " 'hi mandy you are really pretty i love the miley and mandy show write me back pls',\n",
       " 'ciscko s text me if yall going to twitt for our pm meeting',\n",
       " 'huge thanks for the donation rosie really kind and generous of you i m sure will be delighted too x x x',\n",
       " 'pleasures all mine i my friend am doing exceedingly great smile with me',\n",
       " 'the biggest mosquito i ve ever seen just took a chunk out of my smore',\n",
       " 'what i have already read that and made a comment even',\n",
       " 'thanks',\n",
       " 'omg i love halo by beyonce its such a good song',\n",
       " 'i wish someone would bring you too',\n",
       " 'on the train home these are the moments i m gonna remember most yeah i do want to leave best days of my actual life',\n",
       " 'i m honored that you used the photo i made for you on this profile',\n",
       " 'sounds like it',\n",
       " 'seriously give them a try gooey cheese fresh bacon bits butter sour cream chives what s not to love',\n",
       " 'sending big green hugs for the ecomonday',\n",
       " 'damn youuuuuuuuuu lmaooooo y tho whyyyyyyy again no wit about me this morning',\n",
       " 'looks yummy i can haz smal pies pleess',\n",
       " 'good night i am right behind you',\n",
       " 'lmao i knew you d do it my brothers bin helping me with mine he s so clever its actually annoying lmfao xx',\n",
       " 'i am on ie as ff isn t working on this machine i can t reply to people',\n",
       " 'you re right maybe breaks',\n",
       " 'annable b s is in my netflix queue i should receive the first dvd mon can t wait to watch it give a full review ebert style',\n",
       " 'mcc i miss you',\n",
       " 'have agree with u rg yeah we missed out but did you have to remind us donnie',\n",
       " 'haha thanks for helping',\n",
       " 'cna australian h n flu sufferer dies cause uncertain via poor thing',\n",
       " 'awww my little cousin in cute she get it from her daddy',\n",
       " 'doin my nails the tv said names n i typed wat i heard don t u just hate that lol',\n",
       " 'next exams coming up tuesday friday and yours yes i know both dani and youuurs can t wait',\n",
       " 'same tbh the whole film makes me cry all hours of it',\n",
       " 'thank you i should do one of these follow fridays too',\n",
       " 'well get a huge donut',\n",
       " 'i see my name',\n",
       " 'wht the site is i mean its a vote but still dont get the contest part just the recognition i guess voted',\n",
       " 'mrsmcflygrimmy yeah i will be just really gutted at the mo xxx',\n",
       " 'haven t had much of one yet i ll get back to you what have you been up to today besides stalking those on twitter',\n",
       " 'okay i m back to painting will check in with you guys again soon',\n",
       " 'its almost as bad as what i want to call my baby girl if i ever have one magdalene',\n",
       " 'w i think i may have upset nothing intended and feel bad now x',\n",
       " 'i m ok and this isn t the best place to tell aunty jen',\n",
       " 'god this morning s gone fast i hope this afternoon goes just as fast',\n",
       " 'k will reply wen get chance goin out any sec up a tall unfinished building scared of heights too',\n",
       " 'ohhh',\n",
       " 'lol i couldn t help it i was so happy',\n",
       " '',\n",
       " 'omg i thought there was something on my forehead and i scratched then having to find out that it was a scab it started bleeeedin',\n",
       " 'ken s red x or the lovely smell coming through my window',\n",
       " 'louise have a fantastic time you ll have to fill me in when you get back',\n",
       " 'so the nap lasted longer than expected off to glorietta to go shopping',\n",
       " 'just got done watchingg he s just not that into you we all sheded tears whenn does my lifee becomee a movieee',\n",
       " 'thanks this one will be a hit has been a patron since',\n",
       " 'from sway sway baby not all',\n",
       " 'just came home',\n",
       " 'lol hooray hooray suenahmi it is and all credit of the name will b givin to you when pple ask the cats name lol',\n",
       " 'dreaming my day started like houres ago but i had school boring',\n",
       " 'i ve added you let me know if it works it should',\n",
       " 'cro by woo im your first follower',\n",
       " 'this one turned out kind of cute',\n",
       " 'today was national doughnut day i missed out',\n",
       " 'wow bruise on my leg much',\n",
       " 'make a pink black skulls hearts theme haha are any of ur themes posted somewhere id like to check em out',\n",
       " 'beautiful here in nw lower mi today balmy and calm seas on the lake',\n",
       " 'c thinking about pulling a pope joan and pretending to be my brother',\n",
       " 'i m sorry be more careful kays',\n",
       " 'good morning gents how are ya today',\n",
       " 'actually i don t need it i use a mac and have software built in for that',\n",
       " 'this girl on the train boyfriend deaded her over the phone she is siiiiiiiick poor baby',\n",
       " 'spongebob is on come watch with me xx',\n",
       " 'caller again had ppl calling in britneytickets',\n",
       " 'bullying does happen to all ages plenty of people at work get bullied by colleagues',\n",
       " 'oh thanks for explaining that good idea',\n",
       " 'pleasure mate',\n",
       " 'chickmovie night at demi lovato s it s all good fodder for lyrics xoxo',\n",
       " 'got a grade of on check it out whhhooohoo yahooooo and yehhaaa now what',\n",
       " 'so is urs rick your cartoons are awesome',\n",
       " 'home after a lively day at work yea it was a good one tuesday feels like a promising day',\n",
       " 'aw you nice',\n",
       " 'originally or now now near you originally not so near',\n",
       " 'yesss she is',\n",
       " 'as always arrazando daughter in the photo speak to say the more you arraza',\n",
       " 'lost then found',\n",
       " 'might have fixed my ps again for that',\n",
       " 'hiya thanks il have alook',\n",
       " 'this is fairy bread with the crusts on any of you had this we love it here at kids parties',\n",
       " 'i hate getting up at that time is early enough for me',\n",
       " 'cool cool i am soo excited for tomorrow i have a counselor app for mcc',\n",
       " 'someone kidnap me so i dont hav to go back to buffalo moro',\n",
       " 'really glad you came out ok i ve heard the tech has improved lots but always glad when its aok',\n",
       " 'will go and see my couturier be right back',\n",
       " 'fairy x haha oh heck i ll do that too in that case',\n",
       " 'i wont will have to soon tho lol xx',\n",
       " 'i lovedd the shiny suit',\n",
       " 'nice and i like your bedding',\n",
       " 'goodnight',\n",
       " 'thats my album right there freshman year baby you don t forget that year',\n",
       " 'so glad ive made all my deadlines now its time for fun',\n",
       " 'thank you my name is maykat and randomly i can sign into other forums but not crackberry annoying',\n",
       " 'we won t see him on bgt then',\n",
       " 'speaking of fb i can t find you on there',\n",
       " 'so do i',\n",
       " 'as i see it yes',\n",
       " 'xd hmm maybe playing with ardy s balloons',\n",
       " 'sounds like a fun day to mee haha was nt jonas adorable joe is soo funny i love how hes like jimmy hhahahah',\n",
       " 'that s great i m gonna hafta visit their website the nxt time i need a bag',\n",
       " 'damn that sucks how long does lunch go for',\n",
       " 'much crap had to use my actual brain and find it in my memory i m surprised i did to tell u the truth yay brain',\n",
       " 'yeah well what else can i do i ve tried almost everything i can to make it work out',\n",
       " 'i can t get back off to sleep and twitter isn t working right from my phone booo xxxxxx',\n",
       " 'my man ya got a dm reply',\n",
       " 'aww it stopped raining',\n",
       " 'just got home from the trip it was fun but it s very very hot there',\n",
       " 'g morning twitterati a quickie this morning as we get ready to head out to the opener of lucy s softball season',\n",
       " 'i know i wish we could all just fly over to america and watch ones of their shows that would be amazing',\n",
       " 'crezo lol alright i ll stop mentioning your love your kris allen lol he s great though btw are you sure your not',\n",
       " 'that s weird at the beginneng jake s wearing no shirt and before he changes he does lol',\n",
       " 'pray for my friend consolacion god give them the strength to overcome this her birthday is coming soon and we want to see her better',\n",
       " 'definitely hope it gets better then',\n",
       " 'haha yes i do no i haven t but i did just do the stupidst thing ever lol',\n",
       " 'the outfit reminded my of nick j they don t have a curly hair option he even has a ring on oh and sims',\n",
       " 'so there i was dancing to the maine when i trip over my own feet hello bloody lip and swollen ankle',\n",
       " 'urgh work thats no fun',\n",
       " 'and just realized i could add more links he he ha ha',\n",
       " 'hey u how s ur day so far',\n",
       " 'yes yes yes new i could count on my church diva mayo praise him',\n",
       " 'i miss u too',\n",
       " 'no text goes the phone xxxx',\n",
       " 'inlapush um pretty good laughs twitter got quiet on my end too normal people like to sleep apparently',\n",
       " 'eyelash curler i think anyways',\n",
       " 'at heart work followed by college are your plans for the day better than mine lol',\n",
       " 'have to sit near the radio so that the rain doesnt interfere with it grr lol haha',\n",
       " 'here i am haha i miss you today was soo sad',\n",
       " '',\n",
       " 'its daddy s day tomorrow and i dont get see my dad boo',\n",
       " 'i know ive just been so jittery all morning',\n",
       " 'thats no fun',\n",
       " 'good good commenting ccg',\n",
       " 'well i ain t coming to it unless gabe is there is he',\n",
       " 'oops ignore last tweet must get better hang of keyboard shortcuts',\n",
       " 'don t get another awesome friend when you re there i m the only one',\n",
       " 'it was postponed',\n",
       " 'so tired i almost hit someone on the way back sorry unexpected pedestrian',\n",
       " 'noooooo it s am and i m doing nothing',\n",
       " 'awww i can t dm you anymore',\n",
       " 'tell your sister she has my approval to marry him lol',\n",
       " 'sure if you want to call it that',\n",
       " 'i think coming down with somthing feel very thirsty',\n",
       " 'i just got it yesterday and i filed on the last day due',\n",
       " 'th update i love bon jovi there we go perfect',\n",
       " 'haha thanks man',\n",
       " 'i ned to keep busy as all i can think about now is seeing its so close i can hardly believe it',\n",
       " 'to buy or not to buy the iphone g s it s on payg i only paid months ago for this one',\n",
       " 'what a terrible programme i call it jeremy vile too',\n",
       " 'nah i d only seen a picture of that omg that little clip things ace hope your okay',\n",
       " 'the army have responded we ve killed it will go back again in mins lol xxxx',\n",
       " 'sian im doing the takeaway thingy too couldnt be a cooking',\n",
       " 'i love my new car i can t believe i got a car all by my self sorry i will be talking about this for a while',\n",
       " 'haha its okay tom hes got nothing on you you are way more amazing and awsome than him xx',\n",
       " 'and now he answers my bbm be strong denise be very strong iono care how adorable he is blahh',\n",
       " 'ok going to clean the back windows one hell of a job will twitter a pretty clean picture later',\n",
       " 'watching mtv movie awards goo aaaash xd',\n",
       " 'couldn found it yet',\n",
       " 'i have a template to do it from good eh haha',\n",
       " 'skies cos that s what colour my tan is apparently',\n",
       " 'fortunately not down just bit like you i guess only more meh then bleh hv some jelly babies',\n",
       " 'good lord i hope not i m talking about that fine ale called samuel adams',\n",
       " 'u were certainly dashing in more ways than one i was goin to shout i m moanyboot but my cousin slapped me lol bitch',\n",
       " 'how is this not like madoff declaring he won t take anybody else s deposits any longer seriously what s pt of k followers',\n",
       " 'calcarrie girls just sang happy birthday now i can blow out my pasta',\n",
       " 'i wanna watch ppp so bad but my dads watching telly where i recorded it i shall have to watch it on youtube i can t wait no more',\n",
       " 'lol no i think that is so funny i d love to see the whole deck though i hope they used batz maru in it',\n",
       " 'no the best band ever was the beatles they rocked and are still so big today old schools rules xd',\n",
       " 'really woulda loved to of seen jonas brothes and mcfly singing together last night would have been amazing',\n",
       " 'ohhh i c thanks again bb',\n",
       " 'i m tired of having the same hair style everyday',\n",
       " 'weellll i used to fit that description interested in your theory and where you might know me from you never know',\n",
       " 'that wasacool video thanks for sharing of pamala he he i think i spelled your name wrong sorry',\n",
       " 'what a gorgeous pic and now she has a badge it s official bea really is the youngest member of twitstable',\n",
       " 'coz i have to cook and eat alone',\n",
       " 'yeah i know said she was looking',\n",
       " 'really cold here x',\n",
       " 'i love the house definitely a good move',\n",
       " 'with mat',\n",
       " 'grins awwwe thanks but you re definitely more incredible',\n",
       " 'sigh we won t get to hear for a while which sucks',\n",
       " 'you guys are too awesome come to manila please iw',\n",
       " 'so are you planning to take the sticker of best of e home',\n",
       " 'lol thanks i wasn t trying to steal the show',\n",
       " 'yes please give him our loev straight from twitter over the seas in hug form if you will',\n",
       " 'dear twitter ive been neglecting you today im sorry its been a trying day',\n",
       " 'i ve only seen photos and i m told its very green with great diving will island hop one day',\n",
       " 'honestly i have no idea where ya off to anyhow',\n",
       " 'is ill and my dog has started kicking his coat again',\n",
       " 'hoping the same thing hope they live upto the hype',\n",
       " 'it was goin off didnt see you there',\n",
       " 'iamlaetitiafierce is too long',\n",
       " 'and carlos going to the beach with his friends sad face',\n",
       " 'yeah well i like them all',\n",
       " 'i love that people don t know your on twitter unless you tweet cos im kinda really addicted',\n",
       " 'hey once again thank you for those pics of multitasking anoop i just looked at them again and they made me smile',\n",
       " 'yea man',\n",
       " 'lol i ll get him to tweet while i m driving if he lets me he thinks my driving sucks it doesn t though',\n",
       " 'oh alright if we couldn t make it today we try tomorrow ok cause i m having fever and hve a major headache atm',\n",
       " 'sometimes we need a little help from a friend',\n",
       " 'wheeeeeeeeee n gga i m teh besssssst my yummy feels bad still though',\n",
       " 'fancy jumping off the observatory at school on study leave day with me',\n",
       " 'it s working then',\n",
       " 'i agree she s obviously got a great chance of winning i think shaheen has a good chance though i love stavros flatley l',\n",
       " 'oh well it s not the hol house than makes asa it s asa that makes asa even if it s just sa',\n",
       " 'loved the mtv movie awards',\n",
       " 'my tattoo still doesn t look right',\n",
       " 'um me thinks you will have something cute and snuggly to take care of much better option',\n",
       " 'lol embarrassed me infront of the boy i like today lmfao i hate you',\n",
       " 'friend txt wants to go wee wander round shops then i got rachel at no relaxing with monster child lol',\n",
       " 'girls i am outta it then my exams',\n",
       " 'everything keeps bouncing back on msn figures just my luck right',\n",
       " 'yazz but i ve still got the old box but the updated software is nice enuff for now are u goin to sonar den or wut xx',\n",
       " 'morning',\n",
       " 'looking for photo inspiration but everything i want to do required more space than i have',\n",
       " 'hey there how you doing today',\n",
       " 'that s right girl ask him',\n",
       " 'i think i really should stop saying yeeeaaah peeps hate me for doing that',\n",
       " 'we only have more day left and she s votes ahead of gio help me mcflyinmanila mcflyinmanila mcflyinmanila',\n",
       " 'living wow what a difference you looks cute both ways but blonde hair color suits you best im reddish brown',\n",
       " 'uh oh i suddenly feel like i need to know what s happening in the big brother house today i think i m hooked noooo',\n",
       " 'ooh cant watch bgt until my guests have gone meh otherwise i ll know whos won',\n",
       " 'fuck it time for pantera today is draining me and i have a match later',\n",
       " 'hmm didn t think of that',\n",
       " 'veronica i ve got another video for u its just processing on youtube love you xoxoxox',\n",
       " 'might need to try a few times',\n",
       " 'i wish i had a water bed',\n",
       " 'yah ohh wait we ll miss you chuck let s take a moment of silence on behalf of chuck',\n",
       " 'blue thanks hunnie you too kisses p',\n",
       " 'the latter want to go out but have no money',\n",
       " 'he seems like such a smug git even without the fascism',\n",
       " 'eezy i ll definitely keep you posted hoping to go over once baby is born can t wait to do the baby clothes shopping',\n",
       " 'omg no way what part im from vestavia hills hehe the hills of vestavia im leavin nce i get better what part',\n",
       " 'i guess when linux rules the world i ll be cheering on microsoft then',\n",
       " 'at home my back hurts so much i wanna take a nap but guess what the bf s coming over',\n",
       " 'night sweetheart',\n",
       " 'and if he were made in leave form he d be an uplifting tea how appropriate',\n",
       " 'ow i just cut my finger on the cheese grater it s like the tiniest cut but it stings so bad',\n",
       " 'glad you two are around',\n",
       " 'gypsy ha s reaction that wrote to us omg i love him',\n",
       " 'sounds like a very interesting and fun book look forward to it great alan rickman story',\n",
       " 'thx for the followfriday',\n",
       " 'yeeaa me its a shame',\n",
       " 'nananana going to try to find some more curly pix',\n",
       " 'thank you for helping my get my th follower',\n",
       " 'haha well thanks lol you re quite the charmer',\n",
       " 'thanks for the ff',\n",
       " 'so sooooory',\n",
       " 'scratch that watching ai from last year the top group medly always cracks me up',\n",
       " 'i have no weird food reactions too bad cause i have no excuse not to eat bad food',\n",
       " 'what s up hun',\n",
       " 'noooo drop by technorati com',\n",
       " 'i love that tv show',\n",
       " 'think im gonna goooo cos im shattered haha ill be back tomorrow g nighttt tweeters xxxx',\n",
       " 'should be on twitter so i can talk to her i miss her so much',\n",
       " 'well its deader than a dead mans convention in deadsville here so i am off to bed nite nite',\n",
       " 'get followers a day using www tweeterfollow com once you add everyone you are on the train or pay vip',\n",
       " 'i am truly honored',\n",
       " 'thank you pigs the site won t let me vote and that makes pecan sad',\n",
       " 'ahh ok ill let u off then days been abit crap bh havent done much am really tired u xx',\n",
       " 'aww cool glad you enjoyed i ll pop over there and have a look x',\n",
       " 'god i need one now',\n",
       " 'agreed count me in',\n",
       " 'lol they all hate you especially my friends',\n",
       " 'always interesting to me i m a bit of a type it haha over not good',\n",
       " 'yess u take a lot of pix and upload them wanna see everything abt ur tour',\n",
       " 'lol cuteeeeee',\n",
       " 'ooh i smoke a day whacha smokin on over there',\n",
       " 'hi ya just hanginhg out hows your night',\n",
       " 'i wish i could cut down my tv hope you had a great night last night',\n",
       " 'eventful morning for me i screamed and cried like a little girl when i found a tick on my back ick ick ick',\n",
       " 'yepyep they want to do more tests but i m really not sure',\n",
       " 'i know i logged off i m gonna log on later',\n",
       " 'miley follow me please please i really want to talk with you but idk how i wrote you messages on myspace but you never answered',\n",
       " 'i m hungry there is no food',\n",
       " 'in cali i am lost please help me find a good home',\n",
       " 'hey i checked out your youtube videos and you re voice is nice working on a new video soon',\n",
       " 'i feel your pain i ve been there dated that survived truth u need to do what she sayin it will be better on the other side',\n",
       " 'i miss pypso she shoulda taken me on her trip to the cook islands new zealand',\n",
       " 'have h w i cbf doing',\n",
       " 'i woke up and realized that i have minutes to get ready and leave to get to school for exams',\n",
       " 'thank you zar for real',\n",
       " 'i love the original crow movie brandon lee hawt so sad he s gone',\n",
       " 'insl for sure dude hey weve been hanging out for ages anyway cool',\n",
       " 'hi andrew many thanks for the follow',\n",
       " 'sheldon nope i tried it doesn t work',\n",
       " 'morning ish hows everyone',\n",
       " 'you shouldn t be so worried about your followers just twit away',\n",
       " 'lol we wanted c up in d but it was sold out so we saw land of the lost it was ok will ferrel is funny had some funny parts',\n",
       " 'those are really cute hahaha get me a pair haha jkjk',\n",
       " 'haha yeah i hope bradleys still alive',\n",
       " 'damn lost two blog followers today weird and sad',\n",
       " 'tweet me individually',\n",
       " 'shines no prob i d be happy to do whatever design you d like',\n",
       " 'b i go to sleep gotta give a special shout out to my new followers k night night twitta loves',\n",
       " 'dang i need a teleporter lol happy sunday',\n",
       " 'im sorry ur down and im not helping i dont mean to make u more upset sorry',\n",
       " 'i miss talkin to you girl',\n",
       " 'that s so depressing',\n",
       " 'hehehe yeah while i sip this richly deserved mug of hot chocolate',\n",
       " 'hopes i dont get bed sweats tonight please no',\n",
       " 'my poor bubbles cat was just attacked by another cat',\n",
       " 'thank you for maternalhealth support',\n",
       " 'hey there new followers waves and now i warn you about how i tweet like a crazed lunatic when watching sports especially footie',\n",
       " 'depends on which version they thought you the one i know doesn t go like that',\n",
       " 'happy birthday karina not a day goes by that i dont think of you i hope you re watching over us',\n",
       " 'myspacing lol trying not to touch my forearm the laptop it might stick n then pain all over again as i ll have tear it up',\n",
       " 'i left it home',\n",
       " 'get followers a day using www tweeteradder com once you add everyone you are on the train or pay vip',\n",
       " 'paramoure s pic didn t show up',\n",
       " 'it keeps telling me i need to use hex color format i didn t change the way i did it boo',\n",
       " 'birminghams music scene getting a well deserved boost during june please follow',\n",
       " 'oh no i m sorry abt gma she was sick b yes i ll send you good thoughts',\n",
       " 'usa over honduras streets of ceiba will be dead tonight good thing mom and dad flew home today bad night to be us there tonight',\n",
       " 'haha thank you',\n",
       " 'oo go twitfit size skirt and corset i havn t worn in years yay',\n",
       " 'good off to sleep night',\n",
       " 'support you you will go so far i have faith in you you have an amazing family tweeted me once you deserve',\n",
       " 'i want to slowdance with david archuleta by the ocean sands at night with the stars peeking at us',\n",
       " 'oh and i ve sent ups packages to myself instead of customers ms sucks',\n",
       " 'dark in the glow haha she d kill me if she had twitter and saw this xd off work behave twitter b back in the afternoon',\n",
       " 'that s just because she trying to figure out whether or not to call or to save the money and drive you to the er herself',\n",
       " 'sorry about the road burn would a flower help',\n",
       " 'patricia briggs is one of my favourite writes',\n",
       " 'came up lol',\n",
       " 'goodnight everyone have a great saturday hugs',\n",
       " 'its hella hot outside so i am gonna get in the pool',\n",
       " 'oh that is such sad news i m so sorry but so nice that you were there for your friend and kitty isn t suffering hug',\n",
       " 'ooh i didn t know you were here good night sleep well x',\n",
       " 'i miss free well porn on tv sometimes they would forget to scramble the porn stations too',\n",
       " 'desperately waiting for her call i don t have her number yet',\n",
       " 'having a lot of fun in my bed by myself and not in that way bbc radio is my sex sebastian tellier folowed by live lykke li',\n",
       " 'i am kind of feeling some horrible nausea today was fun ahhh i am pumped for this summer',\n",
       " 'feel so weird stomach hurts tests are over finished school for months',\n",
       " 'has someone left bb i ve not watched it since it started i m doing so good xx',\n",
       " 'layne nope i totally forgot about it haha',\n",
       " 'it is unreal that were out it won t hit till summer is over prolly fuck playin ufc may cook some eggs before bed',\n",
       " 'lol you just have to love the irony via',\n",
       " 'lmfaoo i think he has a lil crush thats cool im flattered',\n",
       " 'my eyes are so itchyyyy i really don t want to work anymore this week i want it to be the weekend',\n",
       " 'you re welcome i ll keep my eyes peeled for you hugs',\n",
       " 'i m afraid not unless they reschedule fl dates i hope so just gonna live vicariously through everyone else til then',\n",
       " 'you can never ever have enough nachos i d settle for just the cheese',\n",
       " 'i m so happy for you',\n",
       " 'bored cause michael left me want to talk',\n",
       " 'has not been to koko in a long time and is in need of a good party',\n",
       " 'he was very mean dropped the f word and the s word abused me as if its my fault we have stairs',\n",
       " 'is home and having dinner yummy fish fingers',\n",
       " 'phineas and ferb they say if you love something let it go especially if its a caveman teehee',\n",
       " 'this is random but i am dying to know what s on your i pod would love to know what music you listen to besides live lol',\n",
       " 'people still mail things',\n",
       " 'lol international canadian pre uni prgrm you should check out taylor s site i ve always wanted to go to taylor s haha',\n",
       " 'cleaning up',\n",
       " 'mind you knowing he will be in mud ans slime by now lol',\n",
       " 'i am jealous of the gigs you go to',\n",
       " 'got a seat got a seat bloody restaurant im working at is so disorganised had to run about for hours straight my poor feet',\n",
       " 'i can t wait',\n",
       " 'mc nice tweet ya your name alone sound like a brotha with some skills',\n",
       " 'why isn t photobucket working',\n",
       " 'thanks for the follow',\n",
       " 'jd i hate it when i miss a quiz i m saving up for a personal ufo it will take me at least a year no joke sigh',\n",
       " 'dearangel x thanks to follow me back nice to know you',\n",
       " 'of us pm you me anoop his wife and mindtree right',\n",
       " 'two years ago today i followed an amazing person i never thought such an impression could be made he has forever changed my life',\n",
       " 'well ur gonna borrow it',\n",
       " 'my favorite all time quote by the lovely dorothy',\n",
       " 'where r u tom miss u',\n",
       " 'i just got dropped off at school by',\n",
       " 'im ok think i might be getting sick how are you doing',\n",
       " 'my mums like that too i makes me want to hide',\n",
       " 'i have no idea but i pissed of a niche group of grandmothers tonight',\n",
       " 'that must be one of the most awesome feelings ever my ipod s songs got deleted though the syncing went horribly wrong',\n",
       " 'my imac is once again being slow on sending receiving ajax requests',\n",
       " 'i really thought that was how the tune went',\n",
       " 'god bless all the families and friends of the victims of the air france plane',\n",
       " 'i know i m late but by the time i got home i was sooo tired lol go la magic like i said is a young team they ll have other chances',\n",
       " 'the full head of hair excuse the rest of me',\n",
       " 'i burnt my fingers too unfortunately some idiot lit tealights everywhere and one went out of control i fought with it and lost',\n",
       " 'my cuzin has the same',\n",
       " 'yep we all have though we moved it to it s own machine recently which really helped flying now',\n",
       " 'grrr my twit pic never works x',\n",
       " 'at the zoo',\n",
       " 'i m good work was okay i guess lol do u feel better',\n",
       " 'damm no more pancakes',\n",
       " 'exam done went ok headache massive ow',\n",
       " 'ahhh yes sorry about that',\n",
       " 'gosh ur soo lucky',\n",
       " 'think i got soap in my eyes eyes running like mad not much more twitter tonight good night everyone xxx',\n",
       " 'need to go and steal parents car in a minute wish i could still afford to run mine',\n",
       " 'no your danish fans loves you xx',\n",
       " 'yup last night is it the one that is like a chime',\n",
       " 'wendy s sucks i have minutes left someone come to yogurtime n entertain me',\n",
       " 'oh i m so sorry i hope you will be better soon that s so unfair',\n",
       " 'lol ahh best believe u just don t know what the hell to do either cause i passed by urs yesterday and was like missin',\n",
       " 'oh good morning potato cake how s your dog doing',\n",
       " 'wow for the first time in months i m so glad to hear it that s great i m doing good as well another long weekend',\n",
       " 'i am actually how are you hun',\n",
       " 'my son breakdancing trying to be like',\n",
       " 'until brings back our everyonebutton access pt timeline or link in btm of settings',\n",
       " 'really but the movie because i hate to read haha xd',\n",
       " 'tts nice and i hafta lunch alone cos everyone s out',\n",
       " 'helping support world hunger relief joining the cause see how you can help please retweet',\n",
       " 'my tummy is full my thirst is quenched my layout is complete',\n",
       " 'going to work laterss',\n",
       " 'your luck better than average and it ll probably shout racists stuff while its at it',\n",
       " 'i like home makeover shows boo i wanna watch lol',\n",
       " 'musings good answer girl',\n",
       " 'i think so i know perfect people love them but i would not call them out on a sat night',\n",
       " 'nobody gets how i feel',\n",
       " 'graduation was bomb he didnt wear anything dressy but it was ok jordan looked beautiful in her dress eddie got an itouch woww',\n",
       " 'i m sooo glad that this week is a short one still exhausted all the time',\n",
       " 'awesome for bucks it sounds like a steal',\n",
       " 'those crusty bits are the best but plenty to go around',\n",
       " 'why limit your story to ch tell all what is annoying you at www iamsoannoyed com',\n",
       " 'i feel so sick right now oh no',\n",
       " 'nice glasses looks good',\n",
       " 'og eminem just gave me flashes of listening to it when it came out in melissa s room while doing each other s weaves drinking bud light',\n",
       " 'it is effin hot out here class of will have skin cancer by the end of today but we ll all havea really bomb ass tan glass half full',\n",
       " 'ohh i m already hungry but the is not yet cook aw what should i do',\n",
       " 'i think i can handle that just fine officer my tui actually enables me flirt with guys even better it s like a super power',\n",
       " 'interesting to see pretty graphs show what we suspected let s hope twitter can keep the whale at bay',\n",
       " 'i m on my blackberry so can t listen to the link but if it s shattered dreams ur right',\n",
       " 'edit going to work joy i miss matthew bye x',\n",
       " 'oooh who s gonna be my th follower',\n",
       " 'cara oh thats just great got an ear ache coming on',\n",
       " 'iweb mobileme are pissing me off giving up work on bit ly kittehcam for now',\n",
       " 'i sent mine in hope they awnser it',\n",
       " 'omg school is so annoying it s takin forever i wanna go home i haven t been able to catch up on robsten',\n",
       " 'at work',\n",
       " 'the majestics are on the road to cartmel to play baseball',\n",
       " 'how come you men all know the song wahahaha i like the song i ve been looping it times liao very catchy',\n",
       " 'yup i got my name in like times',\n",
       " 'rgh indeed though i got some cool shots of some of their sharks and things',\n",
       " 'hope you had fun with lisa the jillian replacement lol i was busy getting rosiecakes drunk lol i u both xo',\n",
       " 'i am hella hungryy i can eat a horse',\n",
       " 'haha whoaaa thats awesome haha tomorrow at your school you ll be laughing the whole time haha',\n",
       " 'dutt you are getting senile man oxford as in oxford dictionary',\n",
       " 'i sure do danny fingers x d i feel cool hope you are too gotta dash matey catch ya laters',\n",
       " 'knees starting to get worse again just taking the other tablets the doc gave me at the hospital might get a little spaced out lol',\n",
       " 'starting to make my way back to leeds feeling a bit bummed out sad i didn t get to see my mum',\n",
       " 'hope you had a good day nothing major just spending way too much texting time with someone',\n",
       " 'omgoshh cut me off please i look creepy',\n",
       " 'look u blowin up like the world trade',\n",
       " 'good afternoon yeah we ve been told we ll have rain by friday and it ll last all weekend and possibly into next week too',\n",
       " 'watching mediawatch on unmetered on love it',\n",
       " 'hells kitchen was shit tonight',\n",
       " 'hey sorry your hot and frustrated',\n",
       " 'i think maybe you need to go to the store and buy some food or hit up bk and get your big mac on wait',\n",
       " 'chuck norris is the amrikan equivalent of our mallu jayan',\n",
       " 'took this picture today at my grandmas house aka my old home i miss my nana',\n",
       " 'rd dozen of choc chip cookies r in the oven almost time to need the dough in mins gotta love a busy kitchen doing dishes too',\n",
       " 'sweet see you then',\n",
       " 'no prob glad i could help',\n",
       " 'whoa no kidding i was there too i was eating buttery popcorn so i didn t get to check my phone',\n",
       " 'yup it lovely',\n",
       " 'twittercoffeeclub the first conference call',\n",
       " 'what time boys',\n",
       " 'i saw the sun but then i blink and it was gone',\n",
       " 'watching the grudge',\n",
       " 'i m sorry haha i remembered you cause you said you woke up at am when you took a nap',\n",
       " 'thanks',\n",
       " 'hi elia there you are missed you new day new style new ideas great is the storm still over central us',\n",
       " 'exactly diversity ftw',\n",
       " 'i think she feeling because she cannot figure out how to make her fans and justin co exist peacefully',\n",
       " 'i haven t spoken to my honey so focused in fowever',\n",
       " 'stayed the same let s stick to it',\n",
       " 'facebook photos and video uploads keep crashing',\n",
       " 'hey ya ll thought i shud get kno u awesome gentlemen so here i am sayin hello',\n",
       " 'probably not a good idea its hard to tweet on the web i miss tweetdeck cry',\n",
       " 'rofl lmao lol oooooh my god sorry lol yep that was indeed a long shot xx',\n",
       " 'aww thanks',\n",
       " 'hahaha sweetness i m still listening to it and u should be seriously thanking me cuz that s not a bad thing',\n",
       " 'm r wtf on my way to philly on the nj turnpike still i guess',\n",
       " 'thanks have added bbc rosetta stone websites in to the guide',\n",
       " 'havent played that yet but i hear ya',\n",
       " 'oh and the other yr kid who also happened to be in the library too started downloading portableapps when he found out he could run ffnn',\n",
       " 'omg it was amazing they performed songs of their new cd and i overall loved it lol x',\n",
       " 'we must be the same vintage',\n",
       " 'posted my form already hope you guys like it',\n",
       " 'love this weather it certainly makes you think of foreign holidays',\n",
       " 'much i am lost please help me find a good home',\n",
       " 'get followers a day using www tweeterfollow com once you add everyone you are on the train or pay vip',\n",
       " 'my three year old is screaming lord only knows why can t you see i m watching masterchef darling be a good boy and eat your custard',\n",
       " 'lol u should try the milk too i don t like milk that much but it makes miracles for my throat',\n",
       " 'love you toooooo tg lol gngb',\n",
       " 'i dont have any ugh not even milk',\n",
       " 'try the auteur or hold fire for bands',\n",
       " 'sounds fun unfortunately it s hot humid with no wind here lol',\n",
       " 'urgh its raining baaaaaaaaaad',\n",
       " 'not too much anymore it s slowly leaving me',\n",
       " 'eating cookies yummiii',\n",
       " 'i recorded it on my phone',\n",
       " 'i feel so left out of the party',\n",
       " 'haha i cannt remember how many text updates i got fromm youu sleep well text me later',\n",
       " 'gonna do some family tree stuff now ps pete stop talking about yourself like you re not there',\n",
       " 'yep that s what i am saying i didn t try them all but i remember it being okay not fabulous a can bottles',\n",
       " 'to run him over lmao mmmhm lucky can i join eeeehm why they trying to phone you',\n",
       " 'well you are a charming man after all',\n",
       " 'ugh my stress showed up on my face and of my nails my bunny frou frou is showing',\n",
       " 'grrrrrr i am sick of rain i got up to watch a race not a parade',\n",
       " 'i d like to put your fruity article on my website if you ll permit',\n",
       " 'hehe see u do know about it well done oh hbk luv him',\n",
       " 'wembley is officially over now i m on the train',\n",
       " 'your pic is really cute',\n",
       " 'send something for matt prokop s birthday project pretty please',\n",
       " 'o i wish i could afford bikram everyday',\n",
       " 'was planning on working on customs until pm tonight i won t even be upstairs by then customs will have to wait until tomorrow',\n",
       " 'the guy in my chipper know s my order',\n",
       " 'sleeping early tonight runny noses suck',\n",
       " 'walmart has buy me now perfume in the air',\n",
       " 'dont give up come back to it later with a fresh mind',\n",
       " 'i will die now frustrated with some stupid stuffs can t even stabilize my thoughts',\n",
       " 'never a relaxing weekend for the allahpundit',\n",
       " 'yeah sure',\n",
       " 'thanks it was so good lol me and my best friend were singing the whole time we even got up and started dancing aha',\n",
       " 'could me and come with you lol we are bored with the uk xxx',\n",
       " 'i wanna go to just on friday stoopid fkn work',\n",
       " 'but its a bank holiday here today so not working',\n",
       " 'timothy',\n",
       " 'i wish i could meet you once do u think this will happen someday',\n",
       " 'sounds as if they meant not by may th but by the end of may th doesn t matter let s go for the finish line',\n",
       " 'sry didn t understand what you re asking',\n",
       " 'its almost time for me to go to bed stupid time zones',\n",
       " 'listening to the downtown fiction s ep',\n",
       " 'you are welcome',\n",
       " 'trini bajan lmao who s playing games im shocked and appalled',\n",
       " 'i m kinda really regretting planning this this will be the last time i probally ever see him',\n",
       " 'let s get crazy miley cyrus hannah montana the song that describes my mood the best right now',\n",
       " 'shoulda known that would bring you out of hiding good afternoon pizza for lunch here',\n",
       " 'thank you everyone',\n",
       " 'jk u can come over lunch arrachera arroz frijoles sound good',\n",
       " 'i love your entries and exits make me laugh every time',\n",
       " 'gotta have one if ur in maine eh',\n",
       " 'just wants to stay in bed all day but has to go to work instead sux',\n",
       " 'its ok i was too busy myself but did do some following so that made me feel beter',\n",
       " 'that s fairly clever i think for the most part i m still with the greens but this is a good idea',\n",
       " 'oh gosh too late i m dying i m blind one eye already what u doin now i gotta leave in morn',\n",
       " 'hours then it s horrible old monday again quick better enjoy this while we can',\n",
       " 'aparently his co worker insulted a streetwalker and she threatened them honestly the things he comes home from work saying',\n",
       " 'brisbane is the best baby kill it tonight since i can t be there',\n",
       " 'watt i always go i on lol ty i will have fun at two bands didnt show up',\n",
       " 'is revising and gonna go fetch little sis in a bit come on summer bbq on the park on the th hopefully xx',\n",
       " 'i asked my mom and told her i need an answer soon but i am pretty sure she is leaning towards a yes',\n",
       " 'i actually got it this time sad i ll get no credit',\n",
       " 'just woke up horrible dreams still sleepyyyy zzzz',\n",
       " 'it would be epic fun',\n",
       " 'i think it was on the th night the guys finally lost',\n",
       " 'if either of them reply to you i will do a silly girly squeal and jump up and down for you',\n",
       " 'hey no worries',\n",
       " 'plz say happy birthday or roni mickey plz plz plz',\n",
       " 'omggggggggggg i want to be there so bad boooo',\n",
       " 'im trying im trying',\n",
       " 'still havent opened my bad to do h w maybe i should its like ten to s',\n",
       " 'but u know they won t right',\n",
       " 'would luv to but there are no comps for it',\n",
       " 'i must say i ve been offered worse washes hands',\n",
       " 'ssat stuff costs',\n",
       " 'watching everton v manchester united hopes everton win',\n",
       " 'ilovemovies but i haven t even watched angels demons yet',\n",
       " 'no i dont have net so i cant youtube him',\n",
       " 'i have crampssss',\n",
       " 'thanks for retweeting daly',\n",
       " 'where is my smooch',\n",
       " 'courseeee and youuuu',\n",
       " 'just have fun',\n",
       " 'wow months for holiday my longest holiday just not more than weeks',\n",
       " 'on my way to camden',\n",
       " 'but i m not going this year i am going next year though so the info would be much appreciated',\n",
       " 'teehhheee thankks isoheartssyyoulikewhoa',\n",
       " 'i am so tired i have to walk along a stupid beach countin stpid ciggarette ends in the stupid f ing rain im not in a gd mood',\n",
       " 'get followers a day using www tweeterfollow com once you add everyone you are on the train or pay vip',\n",
       " 'uh lemme lose some pounds before i hit the beach haha been eating way too much and it shows but thanks for the invite',\n",
       " 'dit hehehe would it be weird if i said that i wanted to see it b c paula did',\n",
       " 'sweet thanks maybe i ll have to get caught up between chuckmemondays',\n",
       " 'oh man i would love to see it in hd you re making me so jealous d lol',\n",
       " 'off to some all you can eat place lmao chicken baaallssss please',\n",
       " 'thanks thanks man yea its a skycity apparently they renovated it it looks mean',\n",
       " 'i jst forogt to ask you did you get my txt bout having the money for bec s party lol',\n",
       " 'i know they are both brilliant and stavros flatley lol i love them too',\n",
       " 'as if it s not me that s a bad call kid i take pride in my twiter game',\n",
       " 'i can see the family resemblance in the photo',\n",
       " 'i didn t even get to see them i caught the convo about minutes after it happened',\n",
       " 'really would like to see coraline soon plus maybe synecdoche new york afterwards or before either is good need some time',\n",
       " 'time to get purrtty wink',\n",
       " 'yeah i know maybe we can hook up later tonight or tomorrow i m here til mon i ll text you',\n",
       " 'just seen the total so far for the blisstwits cool',\n",
       " 'thank you followfridayonsundayinstead',\n",
       " 'sad time i might being back down to florida in july',\n",
       " 'there s a whole lot wrong with that',\n",
       " 'i got vids of the singing with donnie jordan was talking about all of it',\n",
       " 'haha true i swear i am hogging the home page for many haha meh',\n",
       " 'i have a mean ass headache i need aspirin now',\n",
       " 'have a lazaaay sunday at homeeeey hoaaaahm i miss him',\n",
       " 'yay i will be glued to my blackberry ya ll have my number too',\n",
       " 'sigh has gone i miss her already but pretty sure we found a hotel',\n",
       " 'logging into album myorli famous password oooh do i need a pass',\n",
       " 'lol i normally do try to avoid any crunchy things in the kid made breakfast todays was pretty good',\n",
       " 'that would ve been a better deal',\n",
       " 'evening will whack it on the ol skydrive now',\n",
       " 'next weeks euromillions is an est m i ll be buying a few tickets',\n",
       " 'with u came to germany thouugh',\n",
       " 'indeed i do now my tummy pains have gone away and now i just have a headache',\n",
       " 'getting ready to hop on a call with and big for k pimpin',\n",
       " 'wow i guess the one where he is smiling sure must be about some happy dream haha',\n",
       " 'hey honey sigh i m scared about tomorrow afraid that i won t get to have any face time with donnie',\n",
       " 'hoare i dispute that xx',\n",
       " 'think you could well be right mate think still have a year left on mine',\n",
       " 'love you years strong',\n",
       " 'will buy that black cocktail dress from tango sooon',\n",
       " 'i wish i was coming to see the show tonight come to liverpool xo',\n",
       " 'abo prize cgft',\n",
       " 'anyone here on last fm add me',\n",
       " 'well a golf day sounds right up the alley of a great birthday for you yay',\n",
       " 'watching the golden girls and reading darcys passions i miss bra arthur',\n",
       " 'holy smokes that is hugeee thanks for sharing',\n",
       " 'can t be bothered going home',\n",
       " 'all i say is my lvatt better come today or hmv should look out for ww lmao thank goodness itunes are reliable',\n",
       " 'every day should be a diamond day',\n",
       " 'says its either iphone or some sony ericsson phone gahhhhh spoiled for choice',\n",
       " 'he does that s awesooooooooomeeeee but how do you knwo it s true',\n",
       " 'it s now fathers day and you haven t picked me yet i kind of want to incessantly bother you squarespace',\n",
       " 'liam is my new hero',\n",
       " 'haha damn right no one messes with our clan',\n",
       " 'ugh i feel like crap today off to the doctors at',\n",
       " 'drop by cali somet ime area code i work for a car wash ill give u a free full car detail waxing everything',\n",
       " 'okay u hurt me real bad there mgs lol such a fanboi',\n",
       " 'yay i guessed right do i get a prize',\n",
       " 'are there any uk schools still on holiday tomorrow or do they all go back might go to alton towers first time in months',\n",
       " 'are you gonna be at demis tour this summer if so what dates',\n",
       " 'hahaha i loove that too',\n",
       " 'tell me when to ring baby i love you xxx',\n",
       " 'what really that sucks',\n",
       " 'torres ooc there was a problem with the account',\n",
       " 'yep it aint a day ending in y if i aint in trouble',\n",
       " 'hang tough ieva millions hugs to you',\n",
       " 'i think you re loving this my friend just confirmed that if you are not following me i can not reply back to you',\n",
       " 'my tummy is bothering me',\n",
       " 'just signed up for teachmeet north east presume tmne on thursday th june anyone else going',\n",
       " 'i love my office when it s dry at least',\n",
       " 'i love your pic where are you and i m great that headache yesterday is gone wooh',\n",
       " 'same here but a great morning anyway',\n",
       " 'fan tks for follow',\n",
       " 'it s now the th and still no iphone',\n",
       " 'i hate megavideo stupid time limits',\n",
       " 'i could help with the motivation what do you need',\n",
       " 'make sure it super tight for chitown see ya there after party my house lol',\n",
       " 'hee hee then i am a trendsetter just now listening to you image a psa so weird',\n",
       " 'aww man i bought a notebook that doesn t have perforated pages i hate that',\n",
       " 'jogging isn t really that cool especially if you ve got a high fever',\n",
       " 'ahh well there is still time at sun is out till haha come to the park where is class u at college',\n",
       " 'its characters',\n",
       " 'watching videos on youtube only got up mins ago and im already bored i need a new life might go call for stef',\n",
       " 'woohoo just got my air stream membership confirmation email i m now at least more nerdier than i was before',\n",
       " 'oh have u created any new builds still looking for new button in facebook and ping fm',\n",
       " 'he s mad at me',\n",
       " 'in the need of a new phone and my blackberry cost too much',\n",
       " 'i think i need a hug',\n",
       " 'but i love my job n i m getting paid doing these errands',\n",
       " 'um idk not super short but above my knee',\n",
       " 'oh whats ur plans today xx',\n",
       " 'it s so sad i know that there s probably so much more in edmonton but i neeeeeeeeeed to buy something lmao',\n",
       " 'i do now thanks to my generation',\n",
       " 'lounging around alllll dayyyyy loveeee itttt',\n",
       " 'off to watch laurence cottle tonight nick his ideas and use them tomorrow',\n",
       " 'lmao you know me',\n",
       " 'thinking about it aren t a deal an overdraft effectively the same these days',\n",
       " 'yyyyyep wanna come over wednesday no one s gonna be home and yeah',\n",
       " 'schoool history science fcs mathh englishh all review for finals except for history and fcs',\n",
       " 'follow join the movement if not your are gonna have un athletic babies w no motor skills who will be weird trekkies',\n",
       " 'the mtv awards are the only time i think about watching mtv',\n",
       " 'yeah those pants are horrible yeah my wife loves the job she loves performing and kids so it s a good environment',\n",
       " 'please support electrik red and sign these thanks',\n",
       " 'good morning',\n",
       " 'it s night time',\n",
       " 'aww that movie was cute stupid happily ever afters',\n",
       " 'updates lol damn i have no more things to write here',\n",
       " 'oh man did you miss excellent seasons pretty good one didn t miss a thing really in the last two',\n",
       " 'i missed so much this summer super wanna make sulit the last month',\n",
       " 'yessss call me or email me im curious about what you are up to',\n",
       " 'caine i know nevermind i have two weeks in october when he ll have to fend for himself its been ages since i ve done a solo gig',\n",
       " 'wright it s very hard i d love to be a guy for a day',\n",
       " 'god i hope that i have fun tomorrow i have a slight case of boredomitis right now lol',\n",
       " 'sitting back and relaxed just had lunch',\n",
       " 'that s good',\n",
       " 'stupid you told your followers to follow people but not me',\n",
       " 'confident i m sorry i have no clue i only know at all of this because of um maybe talk to him',\n",
       " 'i know especially when they remind you of the memories',\n",
       " 'belly happy goobers are the best kind of goobers there are',\n",
       " 'home made pizza from scratch you want some lol',\n",
       " 'rrrrrrrr im going to bed cos im annoyed now',\n",
       " 'trendio shit i didn t buy mark webber cos i thought he would not rise and thus went up',\n",
       " 'take that osbad',\n",
       " 'dm you',\n",
       " 'party time at s work tomorrow though so no drinking for amy',\n",
       " 'i can only say bitch oh ass too i think and kiss my ass',\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = df.text\n",
    "result = []\n",
    "for t in testing:\n",
    "    result.append(tweet_cleaner(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224994"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 77129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s292129/.local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1632: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7768444444444444\n",
      "f1 score:  0.8151665746364808\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(result)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.sentiment, test_size=0.1, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "log_clf.fit(X_train,y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = df.text\n",
    "result = []\n",
    "for t in testing:\n",
    "    result.append(tweet_cleaner_updated(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tf-idf:  (224994, 74696)\n",
      "Accuracy:  0.7769777777777778\n",
      "f1 score:  0.815215790248932\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(result)\n",
    "print(\"Shape after tf-idf: \", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.sentiment, test_size=0.1, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "log_clf.fit(X_train,y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCqRW8qS9ncb"
   },
   "source": [
    "# Dealing with class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "lr = LogisticRegression(max_iter = 1000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TzTBSfEJYEbW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def lr_cv(splits, X, Y, pipeline, average_method):\n",
    "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        lr_fit = pipeline.fit(X.iloc[train], Y.iloc[train])\n",
    "        prediction = lr_fit.predict(X.iloc[test])\n",
    "        scores = lr_fit.score(X.iloc[test],Y.iloc[test])\n",
    "        \n",
    "        accuracy.append(scores * 100)\n",
    "        precision.append(precision_score(Y.iloc[test], prediction, average=average_method)*100)\n",
    "        print('              negative         positive')\n",
    "        print('precision:', precision_score(Y.iloc[test], prediction, average=None))\n",
    "        recall.append(recall_score(Y.iloc[test], prediction, average=average_method)*100)\n",
    "        print('recall:   ', recall_score(Y.iloc[test], prediction, average=None))\n",
    "        f1.append(f1_score(Y.iloc[test], prediction, average=average_method)*100)\n",
    "        print('f1 score: ', f1_score(Y.iloc[test], prediction, average=None))\n",
    "        print('-' * 50)\n",
    "\n",
    "    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
    "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
    "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnwF3-Ney63J",
    "outputId": "4931b23e-f036-4a6d-ad8c-91e331adce55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              negative         positive\n",
      "precision: [0.76599882 0.78707767]\n",
      "recall:    [0.68534824 0.84745698]\n",
      "f1 score:  [0.72343267 0.81615212]\n",
      "--------------------------------------------------\n",
      "              negative         positive\n",
      "precision: [0.7706012 0.7861497]\n",
      "recall:    [0.68186851 0.8521051 ]\n",
      "f1 score:  [0.72352448 0.81779973]\n",
      "--------------------------------------------------\n",
      "              negative         positive\n",
      "precision: [0.76644502 0.78483395]\n",
      "recall:    [0.68061999 0.8488725 ]\n",
      "f1 score:  [0.72098738 0.81559812]\n",
      "--------------------------------------------------\n",
      "              negative         positive\n",
      "precision: [0.76606153 0.78700103]\n",
      "recall:    [0.68520666 0.84752795]\n",
      "f1 score:  [0.72338176 0.81614383]\n",
      "--------------------------------------------------\n",
      "              negative         positive\n",
      "precision: [0.76115006 0.78419269]\n",
      "recall:    [0.68113038 0.84426261]\n",
      "f1 score:  [0.71892042 0.81311973]\n",
      "--------------------------------------------------\n",
      "accuracy: 77.84% (+/- 0.16%)\n",
      "precision: 77.60% (+/- 0.19%)\n",
      "recall: 76.54% (+/- 0.16%)\n",
      "f1 score: 76.89% (+/- 0.16%)\n"
     ]
    }
   ],
   "source": [
    "original_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "lr_cv(5, df.text, df.sentiment, original_pipeline, 'macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV0XmT-P7I2A"
   },
   "source": [
    "We can see that the recall for the negative class is quite low, while the precisions for the negative class are high as. This means the classifier is very picky and does not think many things are negative. All the text it classifies as negative is ~68% of the time really negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3e0y27zA-I9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s292129/.local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1632: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "#SMOTE_pipeline = make_pipeline(tvec, SMOTE(random_state=42), lr)\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "testing_tfidf = vectorizer.fit_transform(df.text)\n",
    "\n",
    "smt = SMOTE(random_state=42, sampling_strategy=1, k_neighbors=1, n_jobs=-1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_resample(testing_tfidf, df.sentiment)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE,y_SMOTE, test_size=0.1, random_state=42)\n",
    "\n",
    "#smt_df = pd.DataFrame(X_SMOTE.todense(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OCIMWM27DeQA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247298, 156563)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_SMOTE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202494, 156563)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7865345733926405\n",
      "f1 score:  0.7973123440199653\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260314, 156563)\n",
      "Accuracy:  0.7884910878918254\n",
      "f1 score:  0.7907100501748517\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "X_ROS, y_ROS = ros.fit_resample(testing_tfidf, df.sentiment)\n",
    "print(X_ROS.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ROS, y_ROS, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"f1 score: \", f1_score(y_pred=y_pred, y_true=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_pipeline = make_pipeline(vectorizer, SMOTE(random_state=42, sampling_strategy=1, k_neighbors=1, n_jobs=-1), lr)\n",
    "lr_cv(5, df.text, df.sentiment, SMOTE_pipeline, 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is believed that SMOTE performs better when combined with undersampling of the majority class, such as random undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> k=1, F1 score: 0.768\n",
      "> k=2, F1 score: 0.768\n",
      "> k=3, F1 score: 0.768\n",
      "> k=4, F1 score: 0.768\n",
      "> k=5, F1 score: 0.768\n",
      "> k=6, F1 score: 0.768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-cfd638e227ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# evaluate pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> k=%d, F1 score: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# grid search k value for SMOTE oversampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define dataset\n",
    "X, y = df.text, df.sentiment\n",
    "# values to evaluate\n",
    "k_values = [1, 2, 3, 4, 5, 6, 7]\n",
    "for k in k_values:\n",
    "\t# define pipeline\n",
    "\tmodel = LogisticRegression(max_iter = 5000, n_jobs=-1)\n",
    "\tover = SMOTE(sampling_strategy=0.9, k_neighbors=k)\n",
    "\tunder = RandomUnderSampler(sampling_strategy=0.9)\n",
    "\tsteps = [('tfidf', vectorizer), ('over', over), ('under', under), ('model', model)]\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\t# evaluate pipeline\n",
    "\tcv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=cv, n_jobs=-1)\n",
    "\tscore = mean(scores)\n",
    "\tprint('> k=%d, F1 score: %.3f' % (k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
